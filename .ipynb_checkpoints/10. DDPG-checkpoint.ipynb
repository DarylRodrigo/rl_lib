{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import pdb\n",
    "\n",
    "from src.utils.OUNoise import OUNoise \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_space, action_space):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.noise = OUNoise(action_space)\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(state_space, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_space),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.head(x)\n",
    "    \n",
    "    def act(self, state, add_noise=True):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        \n",
    "        action = self.forward(state).cpu().data.numpy()\n",
    "        if add_noise:\n",
    "            action += self.noise.noise()\n",
    "\n",
    "        return np.clip(action, -1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_space, action_space):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(state_space, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(64 + action_space, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.single = nn.Sequential(\n",
    "            nn.Linear(state_space + action_space, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, actions):\n",
    "        actions = torch.tensor(actions).float()\n",
    "        x = torch.tensor(x).float()\n",
    "        \n",
    "#         x = self.head(x)\n",
    "#         x = self.body(torch.cat((x, actions), dim=1))\n",
    "        \n",
    "        x = self.single(torch.cat((x, actions), dim=1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment with Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: 3\n",
      "Action space: 1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# env = gym.make(\"MountainCarContinuous-v0\")\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "state_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.shape[0]\n",
    "\n",
    "print(\"State space: {}\".format(state_space))\n",
    "print(\"Action space: {}\".format(action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(state_space, action_space)\n",
    "critic = Critic(state_space, action_space)\n",
    "\n",
    "actor_target = Actor(state_space, action_space)\n",
    "critic_target = Critic(state_space, action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = deque(maxlen=int(buffer_size))\n",
    "        self.Experience = namedtuple(\"experience\", [\"state\", \"next_state\", \"action\", \"reward\", \"done\"])\n",
    "    \n",
    "    def add(self, state, next_state, action, reward, done):\n",
    "        e = self.Experience(state, next_state, action, reward, done)\n",
    "        self.buffer.append(e)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        states = torch.stack([ torch.tensor(exp.state) for exp in samples]).float()\n",
    "        next_states = torch.stack([ torch.tensor(exp.next_state) for exp in samples]).float()\n",
    "        actions = torch.stack([ torch.tensor(exp.action) for exp in samples]).float()\n",
    "        rewards = torch.stack([ torch.tensor(exp.reward) for exp in samples]).float()\n",
    "        dones = torch.stack([ torch.tensor(exp.done) for exp in samples]).float()\n",
    "        \n",
    "        return (states, next_states, actions, rewards, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing loss and updating Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optimiser = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "critic_optimiser = optim.Adam(critic.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn():\n",
    "    states, next_states, actions, rewards, dones = mem.sample(batch_size)\n",
    "    \n",
    "    update_actor(states=states)\n",
    "    \n",
    "    update_critic(\n",
    "        states=states,\n",
    "        next_states=next_states,\n",
    "        actions=actions,\n",
    "        rewards=rewards,\n",
    "        dones=dones\n",
    "    )\n",
    "    \n",
    "    update_target_networks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Update\n",
    "\n",
    "<img src=\"./img/ddpg/actor_update.png\" alt=\"Drawing\" style=\"height: 50px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_actor(states):\n",
    "    actions_pred = actor.act(states.numpy())\n",
    "    loss = -critic(states, actions_pred).mean()\n",
    "    \n",
    "    actor_optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    actor_optimiser.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic Update\n",
    "\n",
    "Critic Loss:\n",
    "<img src=\"./img/ddpg/critic_loss.png\" alt=\"Drawing\" style=\"height: 30px;\"/>\n",
    "\n",
    "Critic $y_i$:\n",
    "<img src=\"./img/ddpg/critic_yi.png\" alt=\"Drawing\" style=\"height: 35px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_critic(states, next_states, actions, rewards, dones):\n",
    "    next_actions = actor_target.act(next_states.numpy())\n",
    "    \n",
    "    y_i =  rewards + ( gamma * critic_target(next_states, next_actions).squeeze() * (1-dones ))\n",
    "    expected_Q = critic(states, actions).squeeze()\n",
    "\n",
    "    loss = F.mse_loss(y_i, expected_Q)\n",
    "    \n",
    "    critic_optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    critic_optimiser.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy Weights Over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_networks():\n",
    "    for target, local in zip(actor_target.parameters(), actor.parameters()):\n",
    "        target.data.copy_(tau*local.data + (1.0-tau)*target.data)\n",
    "        \n",
    "    for target, local in zip(critic_target.parameters(), critic.parameters()):\n",
    "        target.data.copy_(tau*local.data + (1.0-tau)*target.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_e = 50\n",
    "max_t = 200\n",
    "buffer_size = 50000\n",
    "batch_size = 32\n",
    "learn_every = 1\n",
    "\n",
    "gamma = 0.99\n",
    "tau = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = ReplayBuffer(buffer_size)\n",
    "\n",
    "score_log = []\n",
    "score_window = deque(maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsiode: 0.0\tWindow Score: nan\tScore: -1133.2210\n",
      "Epsiode: 5.0\tWindow Score: -1380.9968\tScore: -1162.8464"
     ]
    }
   ],
   "source": [
    "for episode in range(max_e):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    for t in range(max_t):\n",
    "        action = actor.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        score += reward\n",
    "\n",
    "        mem.add(state, next_state, action, reward, done)\n",
    "        \n",
    "        if len(mem) > batch_size and t % learn_every == 0:\n",
    "            learn()\n",
    "\n",
    "        if done:\n",
    "            break;\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    score_log.append(score)\n",
    "    score_window.append(score)\n",
    "    \n",
    "    print(\"\\rEpsiode: {:.1f}\\tWindow Score: {:.4f}\\tScore: {:.4f}\".format(episode, np.mean(score_window), score), end=\"\")\n",
    "    \n",
    "    if (episode % 100 == 0):\n",
    "        print(\"\\rEpsiode: {:.1f}\\tWindow Score: {:.4f}\\tScore: {:.4f}\".format(episode, np.mean(score_window), score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(score_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
