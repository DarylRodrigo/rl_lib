{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.Config import Config\n",
    "from src.utils.Logging import Logger\n",
    "from src.utils.atari_wrappers import wrap_deepmind, make_atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env    = make_atari(env_id)\n",
    "env    = wrap_deepmind(env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation shape: (84, 84, 1)\n",
      "Action space: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"observation shape: {}\".format(env.observation_space.shape))\n",
    "print(\"Action space: {}\".format(env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNQNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        # Call inheritance\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(1234)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # x = F.relu(self.bn1(self.fc1(state)))\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        # return F.tanh(x)\n",
    "        return x\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight 32 84 3, but got 3-dimensional input of size [84, 84, 1] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d9d1a5640585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m84\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 338\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight 32 84 3, but got 3-dimensional input of size [84, 84, 1] instead"
     ]
    }
   ],
   "source": [
    "# network = CNNQNetwork(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "s = torch.from_numpy(env.reset())\n",
    "s.shape\n",
    "\n",
    "conv = nn.Conv2d(84, 32, 3)\n",
    "conv(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        action_size (int): dimension of each action\n",
    "        buffer_size (int): maximum size of buffer\n",
    "        batch_size (int): size of each training batch\n",
    "        seed (int): random seed\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory = deque(maxlen=config.buffer_size)  \n",
    "        self.batch_size = config.batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(config.seed)\n",
    "        self.device = config.device\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done, error=0):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "        weights = torch.ones(len(experiences)).float().to(self.device)\n",
    "        memory_loc = torch.zeros(len(experiences)).float().to(self.device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones, weights, memory_loc)\n",
    "\n",
    "    def n_entries(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size, action_size, config):\n",
    "        \"\"\"\n",
    "        Initialize an Agent object.\n",
    "\n",
    "        state_size (int): dimension of each state\n",
    "        action_size (int): dimension of each action\n",
    "        \"\"\"\n",
    "\n",
    "        if config.model is None:\n",
    "            raise Exception(\"Please select a Model for agent\")\n",
    "        if config.memory is None:\n",
    "            raise Exception(\"Please select Memory for agent\") \n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(self.config.seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = self.config.model(state_size, action_size, self.seed, 64, 64).to(self.config.device)\n",
    "        self.qnetwork_target = self.config.model(state_size, action_size, self.seed, 64, 64).to(self.config.device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=self.config.lr)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = self.config.memory(config)\n",
    "\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "        \n",
    "        # Keep \n",
    "        self.eps = self.config.eps_start\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done) \n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.config.learn_every\n",
    "\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if self.memory.n_entries() > self.config.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.config.gamma)\n",
    "    def anneal_eps(self):\n",
    "        self.eps = max(self.config.eps_end, self.config.eps_decay*self.eps) \n",
    "    \n",
    "    def act(self, state, network_only=False):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "          state (array_like): current state\n",
    "          eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.config.device)\n",
    "\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > self.eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "        gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones, is_weights, idxs = experiences\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "\n",
    "        # Get max action from network\n",
    "        max_next_actions = self.get_max_next_actions(next_states)\n",
    "\n",
    "\n",
    "        # Get max_next_q_values -> .gather(\"dim\", \"index\")\n",
    "        max_next_q_values = self.qnetwork_target(next_states).gather(1, max_next_actions)\n",
    "\n",
    "        # Y^q\n",
    "        Q_targets = rewards + (gamma * max_next_q_values * (1 - dones))\n",
    "\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        # Minimize the loss\n",
    "        # - Optimizer is initilaised with qnetwork_local, so will update that one.\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, self.config.tau)\n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        local_model (PyTorch model): weights will be copied from\n",
    "        target_model (PyTorch model): weights will be copied to\n",
    "        tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "    \n",
    "    def get_max_next_actions(self, next_states):\n",
    "        '''\n",
    "        Passing next_states through network will give array of all action values (in batches)\n",
    "        eg: [[0.25, -0.35], [-0.74, -0.65], ...]\n",
    "        - Detach will avoid gradients being calculated on the variables\n",
    "        - max() gives max value in whole array (0.25)\n",
    "        - max(0) gives max in dim=0 ([0.25, -0.35])\n",
    "        - max(1) gives max in dim=1, therefore: two tensors, one of max values and one of index\n",
    "        eg: \n",
    "        - values=tensor([0.25, -0.65, ...])\n",
    "        - indices=tensor([0, 1, ...])\n",
    "        - we'll want to take the [1] index as that is only the action\n",
    "        eg: tensor([0, 1, ...]) \n",
    "        - unsqueeze allows us to create them into an array that is usuable for next bit (.view(-1,1) would also work)\n",
    "        eg: eg: tensor([[0], [1], ...]) \n",
    "        '''\n",
    "        return self.qnetwork_target(next_states).detach().max(1)[1].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "config.env = gym.make('CartPole-v1')\n",
    "\n",
    "config.win_condition = 195.0\n",
    "config.memory = ReplayBuffer\n",
    "config.model = QNetwork\n",
    "config.print_config()\n",
    "\n",
    "logger = Logger(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, logger):\n",
    "    experiment_start = time.time()\n",
    "    env = config.env\n",
    "    frame = 0\n",
    "\n",
    "    agent = Agent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, config=config)\n",
    "\n",
    "    total_scores_deque = deque(maxlen=100)\n",
    "    total_scores = []\n",
    "\n",
    "\n",
    "    for i_episode in range(1, config.n_episodes+1):\n",
    "        states = env.reset()\n",
    "        scores = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for t in range(config.max_t):\n",
    "            actions = agent.act(states)\n",
    "            next_states, rewards, dones, _ = env.step(np.array(actions))\n",
    "            agent.step(states, actions, rewards, next_states,dones)\n",
    "\n",
    "\n",
    "\n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            number_of_time_steps = t\n",
    "            frame += 1\n",
    "\n",
    "            if np.any(dones):\n",
    "                break \n",
    "\n",
    "        agent.anneal_eps()\n",
    "\n",
    "\n",
    "        # Book Keeping\n",
    "        mean_score = np.mean(scores)\n",
    "        min_score = np.min(scores)\n",
    "        max_score = np.max(scores)\n",
    "\n",
    "        total_scores_deque.append(mean_score)\n",
    "        total_scores.append(mean_score)\n",
    "        total_average_score = np.mean(total_scores_deque)\n",
    "\n",
    "        logger.log_scalar(\"score\", mean_score, i_episode)\n",
    "        logger.log_scalar(\"average_score\", total_average_score, i_episode)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        # print('\\rEpisode {}\\tTotal Average Score (in 100 window): {:.4f}\\tMean: {:.4f}\\tMin: {:.4f}\\tMax: {:.4f}\\tDuration: {:.4f}\\t#TimeSteps: {:.4f}'.format(i_episode, total_average_score, mean_score, min_score, max_score, duration, number_of_time_steps), end=\"\")\n",
    "        print('\\rEpi: {}\\t Frame: {} \\tAverage: {:.4f}\\tMean: {:.4f}\\tDuration: {:.2f}\\t#t_s: {:.1f}'.format(i_episode, frame, total_average_score, mean_score, duration, number_of_time_steps), end=\"\")\n",
    "\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpi: {}\\t Frame: {}\\tAverage Score: {:.4f}\\tMean: {:.4f}\\tDuration: {:.2f}\\t#t_s: {:.1f}'.format(i_episode, frame, total_average_score, mean_score, duration, number_of_time_steps))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), \"{}/checkpoint.pth\".format(logger.log_file_path, date=datetime.datetime.now()))\n",
    "        if config.win_condition is not None and total_average_score > config.win_condition: \n",
    "            print(\"\\nEnvironment Solved in {:.4f} seconds !\".format(time.time() - experiment_start))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), \"{}/checkpoint.pth\".format(logger.log_file_path, date=datetime.datetime.now()))\n",
    "            return \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epi: 100\t Frame: 2165\tAverage Score: 21.6500\tMean: 12.0000\tDuration: 0.02\t#t_s: 11.0\n",
      "Epi: 200\t Frame: 3668\tAverage Score: 15.0300\tMean: 12.0000\tDuration: 0.02\t#t_s: 11.0\n",
      "Epi: 300\t Frame: 5090\tAverage Score: 14.2200\tMean: 10.0000\tDuration: 0.01\t#t_s: 9.0\n",
      "Epi: 400\t Frame: 6344\tAverage Score: 12.5400\tMean: 13.0000\tDuration: 0.02\t#t_s: 12.0\n",
      "Epi: 500\t Frame: 7594\tAverage Score: 12.5000\tMean: 11.0000\tDuration: 0.02\t#t_s: 10.0\n",
      "Epi: 600\t Frame: 8730\tAverage Score: 11.3600\tMean: 10.0000\tDuration: 0.01\t#t_s: 9.0\n",
      "Epi: 700\t Frame: 9982\tAverage Score: 12.5200\tMean: 14.0000\tDuration: 0.02\t#t_s: 13.0\n",
      "Epi: 800\t Frame: 11859\tAverage Score: 18.7700\tMean: 48.0000\tDuration: 0.08\t#t_s: 47.0\n",
      "Epi: 900\t Frame: 16071\tAverage Score: 42.1200\tMean: 128.0000\tDuration: 0.19\t#t_s: 127.0\n",
      "Epi: 982\t Frame: 34727 \tAverage: 197.0200\tMean: 333.0000\tDuration: 0.48\t#t_s: 332.0\n",
      "Environment Solved in 54.6210 seconds !\n"
     ]
    }
   ],
   "source": [
    "train(config, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logger.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "watch(config, logger.log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
