{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import gym\n",
    "from Config import Config\n",
    "# from util import train\n",
    "from Models import ActorCritic\n",
    "from Networks import cnn_head_model, actor_model, critic_model, head_model\n",
    "from Memory import Memory\n",
    "from baselines.common.cmd_util import make_env\n",
    "from baselines.common.atari_wrappers import wrap_deepmind, make_atari\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecEnvWrapper\n",
    "import pdb\n",
    "\n",
    "from PPO import PPOPixel\n",
    "from Networks import cnn_head_model, actor_model, critic_model, head_model\n",
    "from Models import ActorCritic\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import copy\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env_id = \"BreakoutNoFrameskip-v4\"\n",
    "env = make_atari(env_id)\n",
    "env = wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=True, scale=False)\n",
    "\n",
    "# config = Config(gym.make('CartPole-v1'))\n",
    "config = Config(env)\n",
    "\n",
    "config.update_every = 500\n",
    "config.num_learn = 4\n",
    "config.win_condition = 230\n",
    "config.n_episodes = 1000\n",
    "config.max_t = 700\n",
    "\n",
    "config.Memory = Memory\n",
    "config.Model = ActorCritic\n",
    "config.head_model = functools.partial(cnn_head_model, config)\n",
    "config.actor_model = functools.partial(actor_model, config)\n",
    "config.critic_model = functools.partial(critic_model, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Image shape to channels x weight x height\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.transpose(observation, axes=(2, 0, 1))\n",
    "\n",
    "def wrap_pytorch(env):\n",
    "    return ImageToPyTorch(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(gym_id, seed, idx):\n",
    "    def thunk():\n",
    "        env = gym.make(gym_id)\n",
    "       \n",
    "        env = wrap_pytorch(\n",
    "            wrap_deepmind(\n",
    "                env,\n",
    "                clip_rewards=True,\n",
    "                frame_stack=True,\n",
    "                scale=False,\n",
    "            )\n",
    "        )\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(env_id, 123343534, 1)\n",
    "env = env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PPO import PPOPixel\n",
    "from Networks import cnn_head_model, actor_model, critic_model, head_model\n",
    "from Models import ActorCritic\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import copy\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# env = copy.deepcopy(config.env)\n",
    "steps = 0\n",
    "scores_deque = deque(maxlen=100)\n",
    "scores = []\n",
    "average_scores = []\n",
    "max_score = -np.Inf\n",
    "\n",
    "agent = PPOPixel(config)\n",
    "\n",
    "state = env.reset()\n",
    "score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# obs = torch.zeros((num_steps, 1) + env.observation_space.shape)\n",
    "# actions = torch.zeros((num_steps,1 ) + env.action_space.shape)\n",
    "# logprobs = torch.zeros((num_steps,1))\n",
    "# rewards = torch.zeros((num_steps,1))\n",
    "# dones = torch.zeros((num_steps,1))\n",
    "# values = torch.zeros((num_steps,1))\n",
    "\n",
    "# obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 84, 84])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(env.reset()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Models import ActorCritic\n",
    "\n",
    "# score = 0\n",
    "\n",
    "# num_steps = 3\n",
    "\n",
    "# obs = torch.zeros((num_steps, 1, 4, 84, 84))\n",
    "# actions = torch.zeros(num_steps, 1)\n",
    "# logsprobs = torch.zeros(num_steps, 1)\n",
    "# rewards = torch.zeros(num_steps)\n",
    "# dones = torch.zeros(num_steps)\n",
    "# values = torch.zeros(num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COSTA\n",
    "class Scale(nn.Module):\n",
    "    def __init__(self, scale):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.scale\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, frames=4):\n",
    "        super(Agent, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            Scale(1/255),\n",
    "            layer_init(nn.Conv2d(frames, 32, 8, stride=4)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(32, 64, 4, stride=2)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            layer_init(nn.Linear(3136, 512)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = layer_init(nn.Linear(512, envs.action_space.n), std=0.01)\n",
    "        self.critic = layer_init(nn.Linear(512, 1), std=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "    def get_action(self, x, action=None):\n",
    "        logits = self.actor(self.forward(x))\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy()\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(self.forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 4, 84, 84])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3838],\n",
       "        [-1.3838],\n",
       "        [-1.3873],\n",
       "        [-1.3891],\n",
       "        [-1.3891],\n",
       "        [-1.3891],\n",
       "        [-1.3891],\n",
       "        [-1.3873],\n",
       "        [-1.3838],\n",
       "        [-1.3891]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Models import ActorCritic\n",
    "\n",
    "model = Agent(env)\n",
    "\n",
    "num_steps = 10\n",
    "\n",
    "obs = torch.zeros((num_steps, 1) + env.observation_space.shape)\n",
    "print(obs.shape)\n",
    "actions = torch.zeros(num_steps, 1)\n",
    "logsprobs = torch.zeros(num_steps, 1)\n",
    "rewards = torch.zeros(num_steps)\n",
    "dones = torch.zeros(num_steps)\n",
    "values = torch.zeros(num_steps)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "for t in range(num_steps):\n",
    "    steps += 1\n",
    "\n",
    "    # Shape obs\n",
    "    obs[t] = torch.FloatTensor(state)\n",
    "    \n",
    "    # Get action & save\n",
    "    with torch.no_grad():\n",
    "        action, log_prob, entr = model.get_action(obs[t])\n",
    "    actions[t] = action\n",
    "    logsprobs[t] = log_prob\n",
    "    \n",
    "    # Act logic & save\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    rewards[t] = reward\n",
    "    dones[t] = done\n",
    "    \n",
    "    state = next_state\n",
    "\n",
    "    # Update \n",
    "    score += reward\n",
    "logsprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape obs: torch.Size([10, 1, 4, 84, 84])\n",
      "Shape action: torch.Size([10, 1])\n",
      "Shape log probs: torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape obs: {}\".format(obs.shape))\n",
    "print(\"Shape action: {}\".format(actions.shape))\n",
    "print(\"Shape log probs: {}\".format(logsprobs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaped state space: torch.Size([10, 4, 84, 84])\n",
      "reshaped state space: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "b_obs = obs.reshape((-1,)+env.observation_space.shape)\n",
    "# b_prev_state = obs.squeeze(dim=1)\n",
    "b_actions = actions.reshape((-1,)+env.action_space.shape)\n",
    "b_logprobs = logsprobs.reshape(-1)\n",
    "\n",
    "print(\"reshaped state space: {}\".format(b_obs.shape))\n",
    "print(\"reshaped state space: {}\".format(b_actions.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, lp, e = model.get_action(b_obs, b_actions)\n",
    "lp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = torch.exp(b_logprobs - lp.detach())\n",
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Daryl test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"Some Information about ActorCritic\"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, self.config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor.add_module(\n",
    "            \"actor_linear\",\n",
    "            nn.Linear(config.hidden_size, config.action_space)\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.critic.add_module(\n",
    "            \"critic_linear\",\n",
    "            nn.Linear(config.hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def get_action(self, x, action=None):\n",
    "        logits = self.actor(self.forward(x))\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0008, 1.0018, 0.9995, 0.9989, 1.0007, 0.9994, 0.9978, 0.9983, 1.0021,\n",
       "        1.0006])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = Agent(env)\n",
    "model = ActorCritic(config)\n",
    "\n",
    "num_steps = 10\n",
    "\n",
    "obs = torch.zeros((num_steps, 1) + env.observation_space.shape)\n",
    "actions = torch.zeros(num_steps, 1)\n",
    "logsprobs = torch.zeros(num_steps, 1)\n",
    "rewards = torch.zeros(num_steps)\n",
    "dones = torch.zeros(num_steps)\n",
    "values = torch.zeros(num_steps)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "for t in range(num_steps):\n",
    "    steps += 1\n",
    "\n",
    "    # Shape obs\n",
    "    obs[t] = torch.FloatTensor(state)\n",
    "    \n",
    "    # Get action & save\n",
    "    with torch.no_grad():\n",
    "        action, log_prob, entr = model.get_action(obs[t])\n",
    "    actions[t] = action\n",
    "    logsprobs[t] = log_prob\n",
    "    \n",
    "    # Act logic & save\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    rewards[t] = reward\n",
    "    dones[t] = done\n",
    "    \n",
    "    state = next_state\n",
    "\n",
    "    # Update \n",
    "    score += reward\n",
    "    \n",
    "b_obs = obs.reshape((-1,)+env.observation_space.shape)\n",
    "b_actions = actions.reshape((-1,)+env.action_space.shape)\n",
    "b_logprobs = logsprobs.reshape(-1)\n",
    "\n",
    "a, lp, e = model.get_action(b_obs, b_actions)\n",
    "ratio = torch.exp(b_logprobs - lp.detach())\n",
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train-procgen",
   "language": "python",
   "name": "train-procgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
