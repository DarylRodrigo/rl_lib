{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import gym\n",
    "from Config import Config\n",
    "# from util import train\n",
    "from Models import ActorCritic\n",
    "from Networks import cnn_head_model, actor_model, critic_model, head_model\n",
    "from Memory import Memory\n",
    "from baselines.common.cmd_util import make_env\n",
    "from baselines.common.atari_wrappers import wrap_deepmind, make_atari\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(3,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CarRacing-v0')\n",
    "env.observation_space.shape\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(gym.make('CarRacing-v0'))\n",
    "config = Config(env)\n",
    "\n",
    "config.update_every = 500\n",
    "config.num_learn = 4\n",
    "config.win_condition = 230\n",
    "config.n_episodes = 1000\n",
    "config.max_t = 700\n",
    "\n",
    "config.Memory = Memory\n",
    "config.Model = ActorCritic\n",
    "config.head_model = functools.partial(cnn_head_model, config)\n",
    "config.actor_model = functools.partial(actor_model, config)\n",
    "config.critic_model = functools.partial(critic_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "env = copy.deepcopy(config.env)\n",
    "state = env.reset()\n",
    "print(torch.FloatTensor(state).shape)\n",
    "# agent = PPO(config)\n",
    "\n",
    "# action, log_prob = agent.act(torch.FloatTensor(state))\n",
    "# print(\"[unsqueezed] Action space shape: {}\".format(action.shape))\n",
    "# print(\"[unsqueezed] Log Probabilities: {}\".format(log_prob))\n",
    "\n",
    "# action, log_prob = agent.act(torch.FloatTensor(state))\n",
    "# print(\"[unsqueezed] Action space shape: {}\".format(action.shape))\n",
    "# print(\"[unsqueezed] Log Probabilities: {}\".format(log_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from PPO import PPO\n",
    "from Config import Config\n",
    "import pdb\n",
    "\n",
    "def get_state(obs):\n",
    "    state = np.array(obs)\n",
    "    state = state.transpose((2, 0, 1))\n",
    "    state = torch.FloatTensor(state)\n",
    "    return state.unsqueeze(0)\n",
    "\n",
    "def get_save_state(obs):\n",
    "    state = np.array(obs)\n",
    "    state = state.transpose((2, 0, 1))\n",
    "    state = torch.FloatTensor(state)\n",
    "    return state\n",
    "\n",
    "def train(config):\n",
    "    env = copy.deepcopy(config.env)\n",
    "    steps = 0\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    average_scores = []\n",
    "    max_score = -np.Inf\n",
    "\n",
    "    agent = PPO(config)\n",
    "\n",
    "    for i_episode in range(1, config.n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(config.max_t):\n",
    "            steps += 1\n",
    "\n",
    "            action, log_prob = agent.act(get_state(state))\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            agent.mem.add(get_save_state(state), action, reward, log_prob, done)\n",
    "\n",
    "            # Update \n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "\n",
    "            if steps >= config.update_every:\n",
    "                agent.learn(config.num_learn)\n",
    "                agent.mem.clear()\n",
    "                steps = 0\n",
    "\n",
    "            if done:\n",
    "                break \n",
    "\n",
    "        # Book Keeping\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        average_scores.append(np.mean(scores_deque))\n",
    "\n",
    "        if i_episode % 10 == 0:\n",
    "            print(\"\\rEpisode {}\tAverage Score: {:.2f}\tScore: {:.2f}\".format(i_episode, np.mean(scores_deque), score), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}\tAverage Score: {:.2f}\".format(i_episode, np.mean(scores_deque)))   \n",
    "\n",
    "        if np.mean(scores_deque) > config.win_condition:\n",
    "            print(\"\\nEnvironment Solved!\")\n",
    "            break\n",
    "\n",
    "    return scores, average_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, average_score = train(config)\n",
    "plt.plot(scores)\n",
    "plt.plot(average_score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_state(obs):\n",
    "#     state = np.array(obs)\n",
    "#     state = state.transpose((2, 0, 1))\n",
    "#     state = torch.FloatTensor(state)\n",
    "#     return state.unsqueeze(0)\n",
    "\n",
    "# def get_state_save(obs):\n",
    "#     state = np.array(obs)\n",
    "#     state = state.transpose((2, 0, 1))\n",
    "#     state = torch.FloatTensor(state)\n",
    "#     return state\n",
    "\n",
    "# env = copy.deepcopy(config.env)\n",
    "# steps = 0\n",
    "# scores_deque = deque(maxlen=100)\n",
    "# scores = []\n",
    "# average_scores = []\n",
    "# max_score = -np.Inf\n",
    "\n",
    "# agent = PPO(config)\n",
    "\n",
    "# for i_episode in range(1, config.n_episodes+1):\n",
    "#     state = env.reset()\n",
    "#     score = 0\n",
    "#     for t in range(config.max_t):\n",
    "#         steps += 1\n",
    "\n",
    "#         action, log_prob = agent.act(get_state(state))\n",
    "# #             print(\"Action space shape: {}\".format(action.shape))\n",
    "# #             print(\"Action: {}\".format(action))\n",
    "# #             print(\"Log Probabilities: {}\".format(log_prob))\n",
    "# #             print(\"Action item: {}\".format(action.item()))\n",
    "#         next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "#         agent.mem.add(get_state_save(state), action, reward, log_prob, done)\n",
    "\n",
    "#         # Update \n",
    "#         state = next_state\n",
    "#         score += reward\n",
    "\n",
    "\n",
    "#         if steps >= config.update_every:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev_states = torch.stack(agent.mem.states).to(agent.device).detach()\n",
    "# prev_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action, log_prob = agent.act(prev_states)\n",
    "# action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset()\n",
    "# state = get_state(state)\n",
    "# print(state.shape)\n",
    "# action, log_prob = agent.act(state)\n",
    "# action.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train-procgen",
   "language": "python",
   "name": "train-procgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
