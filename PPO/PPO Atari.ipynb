{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import gym\n",
    "from Config import Config\n",
    "# from util import train\n",
    "from Models import ActorCritic\n",
    "from Networks import cnn_head_model, actor_model, critic_model, head_model\n",
    "from Memory import Memory\n",
    "from baselines.common.cmd_util import make_env\n",
    "from baselines.common.atari_wrappers import wrap_deepmind, make_atari\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env_id = \"BreakoutNoFrameskip-v4\"\n",
    "env = make_atari(env_id)\n",
    "env = wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=True, scale=False)\n",
    "\n",
    "# config = Config(gym.make('CartPole-v1'))\n",
    "config = Config(env)\n",
    "\n",
    "config.update_every = 500\n",
    "config.num_learn = 4\n",
    "config.win_condition = 230\n",
    "config.n_episodes = 1000\n",
    "config.max_t = 700\n",
    "\n",
    "config.Memory = Memory\n",
    "config.Model = ActorCritic\n",
    "config.head_model = functools.partial(cnn_head_model, config)\n",
    "# config.head_model = functools.partial(head_model, config)\n",
    "config.actor_model = functools.partial(actor_model, config)\n",
    "config.critic_model = functools.partial(critic_model, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([84, 84, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "env = copy.deepcopy(config.env)\n",
    "state = env.reset()\n",
    "print(torch.FloatTensor(state).shape)\n",
    "# agent = PPO(config)\n",
    "\n",
    "# action, log_prob = agent.act(torch.FloatTensor(state))\n",
    "# print(\"[unsqueezed] Action space shape: {}\".format(action.shape))\n",
    "# print(\"[unsqueezed] Log Probabilities: {}\".format(log_prob))\n",
    "\n",
    "# action, log_prob = agent.act(torch.FloatTensor(state))\n",
    "# print(\"[unsqueezed] Action space shape: {}\".format(action.shape))\n",
    "# print(\"[unsqueezed] Log Probabilities: {}\".format(log_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from PPO import PPO\n",
    "from Config import Config\n",
    "import pdb\n",
    "\n",
    "def get_state(obs):\n",
    "    state = np.array(obs)\n",
    "    state = state.transpose((2, 0, 1))\n",
    "    state = torch.FloatTensor(state)\n",
    "    return state.unsqueeze(0)\n",
    "\n",
    "def get_save_state(obs):\n",
    "    state = np.array(obs)\n",
    "    state = state.transpose((2, 0, 1))\n",
    "    state = torch.FloatTensor(state)\n",
    "    return state\n",
    "\n",
    "def train(config):\n",
    "    env = copy.deepcopy(config.env)\n",
    "    steps = 0\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    average_scores = []\n",
    "    max_score = -np.Inf\n",
    "\n",
    "    agent = PPO(config)\n",
    "\n",
    "    for i_episode in range(1, config.n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(config.max_t):\n",
    "            steps += 1\n",
    "\n",
    "            action, log_prob = agent.act(get_state(state))\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            agent.mem.add(get_save_state(state), action, reward, log_prob, done)\n",
    "\n",
    "            # Update \n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "\n",
    "            if steps >= config.update_every:\n",
    "                agent.learn(config.num_learn)\n",
    "                agent.mem.clear()\n",
    "                steps = 0\n",
    "\n",
    "            if done:\n",
    "                break \n",
    "\n",
    "        # Book Keeping\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        average_scores.append(np.mean(scores_deque))\n",
    "\n",
    "        if i_episode % 10 == 0:\n",
    "            print(\"\\rEpisode {}\tAverage Score: {:.2f}\tScore: {:.2f}\".format(i_episode, np.mean(scores_deque), score), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}\tAverage Score: {:.2f}\".format(i_episode, np.mean(scores_deque)))   \n",
    "\n",
    "        if np.mean(scores_deque) > config.win_condition:\n",
    "            print(\"\\nEnvironment Solved!\")\n",
    "            break\n",
    "\n",
    "    return scores, average_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: 0.10\tScore: 0.00> \u001b[0;32m/Users/darylrodrigo/Documents/projects/rl_lib/PPO/PPO.py\u001b[0m(60)\u001b[0;36mlearn\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     58 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     59 \u001b[0;31m      \u001b[0;31m# calculate advantage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 60 \u001b[0;31m      \u001b[0madvantage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscounted_returns\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     61 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     62 \u001b[0;31m      \u001b[0;31m# calculate surrogates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> actions.shape\n",
      "torch.Size([500, 1])\n",
      "ipdb> exit\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8c7257d674b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-9156ce3febc4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_every\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_learn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/rl_lib/PPO/PPO.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, num_learn)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0;31m# calculate advantage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m       \u001b[0madvantage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscounted_returns\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0;31m# calculate surrogates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/rl_lib/PPO/PPO.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, num_learn)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0;31m# calculate advantage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m       \u001b[0madvantage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscounted_returns\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0;31m# calculate surrogates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/train-procgen/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/train-procgen/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores, average_score = train(config)\n",
    "plt.plot(scores)\n",
    "plt.plot(average_score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_state(obs):\n",
    "#     state = np.array(obs)\n",
    "#     state = state.transpose((2, 0, 1))\n",
    "#     state = torch.FloatTensor(state)\n",
    "#     return state.unsqueeze(0)\n",
    "\n",
    "# def get_state_save(obs):\n",
    "#     state = np.array(obs)\n",
    "#     state = state.transpose((2, 0, 1))\n",
    "#     state = torch.FloatTensor(state)\n",
    "#     return state\n",
    "\n",
    "# env = copy.deepcopy(config.env)\n",
    "# steps = 0\n",
    "# scores_deque = deque(maxlen=100)\n",
    "# scores = []\n",
    "# average_scores = []\n",
    "# max_score = -np.Inf\n",
    "\n",
    "# agent = PPO(config)\n",
    "\n",
    "# for i_episode in range(1, config.n_episodes+1):\n",
    "#     state = env.reset()\n",
    "#     score = 0\n",
    "#     for t in range(config.max_t):\n",
    "#         steps += 1\n",
    "\n",
    "#         action, log_prob = agent.act(get_state(state))\n",
    "# #             print(\"Action space shape: {}\".format(action.shape))\n",
    "# #             print(\"Action: {}\".format(action))\n",
    "# #             print(\"Log Probabilities: {}\".format(log_prob))\n",
    "# #             print(\"Action item: {}\".format(action.item()))\n",
    "#         next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "#         agent.mem.add(get_state_save(state), action, reward, log_prob, done)\n",
    "\n",
    "#         # Update \n",
    "#         state = next_state\n",
    "#         score += reward\n",
    "\n",
    "\n",
    "#         if steps >= config.update_every:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev_states = torch.stack(agent.mem.states).to(agent.device).detach()\n",
    "# prev_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action, log_prob = agent.act(prev_states)\n",
    "# action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset()\n",
    "# state = get_state(state)\n",
    "# print(state.shape)\n",
    "# action, log_prob = agent.act(state)\n",
    "# action.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train-procgen",
   "language": "python",
   "name": "train-procgen"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
