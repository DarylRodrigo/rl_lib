{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import gym\n",
    "from Config import Config\n",
    "# from util import train\n",
    "from Models import ActorCritic\n",
    "from Networks import cnn_head_model, actor_model, critic_model, head_model\n",
    "from Memory import Memory\n",
    "from baselines.common.cmd_util import make_env\n",
    "from baselines.common.atari_wrappers import wrap_deepmind, make_atari\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecEnvWrapper\n",
    "import pdb\n",
    "\n",
    "from PPO import PPOPixel\n",
    "from Networks import cnn_head_model, actor_model, critic_model, head_model\n",
    "from Models import ActorCritic\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import copy\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env_id = \"BreakoutNoFrameskip-v4\"\n",
    "env = make_atari(env_id)\n",
    "env = wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=True, scale=False)\n",
    "\n",
    "# config = Config(gym.make('CartPole-v1'))\n",
    "config = Config(env)\n",
    "\n",
    "config.update_every = 500\n",
    "config.num_learn = 4\n",
    "config.win_condition = 230\n",
    "config.n_episodes = 1000\n",
    "config.max_t = 700\n",
    "\n",
    "config.Memory = Memory\n",
    "config.Model = ActorCritic\n",
    "config.head_model = functools.partial(cnn_head_model, config)\n",
    "config.actor_model = functools.partial(actor_model, config)\n",
    "config.critic_model = functools.partial(critic_model, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Image shape to channels x weight x height\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.transpose(observation, axes=(2, 0, 1))\n",
    "\n",
    "def wrap_pytorch(env):\n",
    "    return ImageToPyTorch(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(gym_id, seed, idx):\n",
    "    def thunk():\n",
    "        env = gym.make(gym_id)\n",
    "       \n",
    "        env = wrap_pytorch(\n",
    "            wrap_deepmind(\n",
    "                env,\n",
    "                clip_rewards=True,\n",
    "                frame_stack=True,\n",
    "                scale=False,\n",
    "            )\n",
    "        )\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(env_id, 123343534, 1)\n",
    "env = env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â PPO test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PPO import PPOPixel\n",
    "from Networks import cnn_head_model, actor_model, critic_model, head_model\n",
    "from Models import ActorCritic\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import copy\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# env = copy.deepcopy(config.env)\n",
    "steps = 0\n",
    "scores_deque = deque(maxlen=100)\n",
    "scores = []\n",
    "average_scores = []\n",
    "max_score = -np.Inf\n",
    "\n",
    "agent = PPOPixel(config)\n",
    "\n",
    "state = env.reset()\n",
    "score = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Even more broken down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Costa agent\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, frames=4):\n",
    "        super(Agent, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            Scale(1/255),\n",
    "            layer_init(nn.Conv2d(frames, 32, 8, stride=4)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(32, 64, 4, stride=2)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            layer_init(nn.Linear(3136, 512)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = layer_init(nn.Linear(512, envs.action_space.n), std=0.01)\n",
    "        self.critic = layer_init(nn.Linear(512, 1), std=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "    def get_action(self, x, action=None):\n",
    "        logits = self.actor(self.forward(x))\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy()\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(self.forward(x))\n",
    "class Scale(nn.Module):\n",
    "    def __init__(self, scale):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.scale\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Original\n",
    "class ActorCritic(nn.Module):\n",
    "  \"\"\"Some Information about ActorCritic\"\"\"\n",
    "  def __init__(self, config):\n",
    "    super(ActorCritic, self).__init__()\n",
    "\n",
    "    self.head = config.head_model()\n",
    "\n",
    "    self.actor = config.actor_model()\n",
    "    self.actor.add_module(\n",
    "      \"actor_linear\",\n",
    "      nn.Linear(config.hidden_size, config.action_space)\n",
    "    )\n",
    "\n",
    "    self.critic = config.critic_model()\n",
    "    self.critic.add_module(\n",
    "      \"critic_linear\",\n",
    "      nn.Linear(config.hidden_size, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.head(x)\n",
    "    value = self.critic(x)\n",
    "    action = self.actor(x)\n",
    "    return action, value\n",
    "  \n",
    "  def act(self, x, action=None):\n",
    "    logits, value = self.forward(x)\n",
    "    probs = Categorical(logits=logits)\n",
    "    if action is None:\n",
    "      action = probs.sample()\n",
    "    return action, probs.log_prob(action), value, probs.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models import ActorCritic\n",
    "\n",
    "\n",
    "model = ActorCritic(config)\n",
    "# Costa\n",
    "model = Agent(env)\n",
    "\n",
    "num_steps = 10\n",
    "\n",
    "obs = torch.zeros((num_steps, 1) + env.observation_space.shape)\n",
    "actions = torch.zeros(num_steps, 1)\n",
    "logsprobs = torch.zeros(num_steps, 1)\n",
    "rewards = torch.zeros(num_steps)\n",
    "dones = torch.zeros(num_steps)\n",
    "values = torch.zeros(num_steps)\n",
    "\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-3.6138e-03, -1.0761e-03, -3.0479e-04, -3.6483e-05]])\n",
      "Logits: tensor([[-3.7068e-03, -9.3832e-04, -2.6323e-04, -2.9061e-05]])\n",
      "Logits: tensor([[-3.7361e-03, -9.0065e-04, -3.2370e-04, -6.8040e-05]])\n",
      "Logits: tensor([[-3.6673e-03, -9.8780e-04, -3.8701e-04, -8.8592e-07]])\n",
      "Logits: tensor([[-3.7360e-03, -9.8863e-04, -3.7301e-04, -2.3885e-05]])\n",
      "Logits: tensor([[-3.6674e-03, -9.6776e-04, -3.1391e-04,  6.1858e-05]])\n",
      "Logits: tensor([[-3.6810e-03, -9.2228e-04, -4.1201e-04, -5.6614e-05]])\n",
      "Logits: tensor([[-0.0037, -0.0009, -0.0004, -0.0002]])\n",
      "Logits: tensor([[-0.0037, -0.0009, -0.0004, -0.0002]])\n",
      "Logits: tensor([[-0.0037, -0.0009, -0.0004, -0.0002]])\n",
      "tensor([[-1.3853],\n",
      "        [-1.3888],\n",
      "        [-1.3851],\n",
      "        [-1.3860],\n",
      "        [-1.3850],\n",
      "        [-1.3850],\n",
      "        [-1.3854],\n",
      "        [-1.3852],\n",
      "        [-1.3859],\n",
      "        [-1.3887]])\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for t in range(num_steps):\n",
    "    steps += 1\n",
    "\n",
    "    # Shape obs\n",
    "    obs[t] = torch.FloatTensor(state)\n",
    "    \n",
    "    # Get action & save\n",
    "    with torch.no_grad():\n",
    "        # Original\n",
    "#         logits, value = model(obs[t])\n",
    "        # costa\n",
    "        logits = model.actor(model(obs[t]))\n",
    "        print(\"Logits: {}\".format(logits))\n",
    "        probs = Categorical(logits=logits)\n",
    "        action = probs.sample()\n",
    "\n",
    "    actions[t] = action\n",
    "    logsprobs[t] = probs.log_prob(action)\n",
    "    \n",
    "    # Act logic & save\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    rewards[t] = reward\n",
    "    dones[t] = done\n",
    "    \n",
    "    state = next_state\n",
    "\n",
    "    # Update \n",
    "    score += reward\n",
    "\n",
    "print(logsprobs)\n",
    "print(logsprobs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaped state space: torch.Size([10, 4, 84, 84])\n",
      "reshaped state space: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "b_obs = obs.reshape((-1,)+env.observation_space.shape)\n",
    "# b_prev_state = obs.squeeze(dim=1)\n",
    "b_actions = actions.reshape((-1,)+env.action_space.shape)\n",
    "\n",
    "print(\"reshaped state space: {}\".format(b_obs.shape))\n",
    "print(\"reshaped state space: {}\".format(b_actions.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.6138e-03, -1.0761e-03, -3.0479e-04, -3.6482e-05],\n",
       "        [-3.7068e-03, -9.3832e-04, -2.6323e-04, -2.9062e-05],\n",
       "        [-3.7361e-03, -9.0065e-04, -3.2370e-04, -6.8039e-05],\n",
       "        [-3.6673e-03, -9.8780e-04, -3.8701e-04, -8.8499e-07],\n",
       "        [-3.7360e-03, -9.8863e-04, -3.7301e-04, -2.3887e-05],\n",
       "        [-3.6674e-03, -9.6776e-04, -3.1391e-04,  6.1858e-05],\n",
       "        [-3.6810e-03, -9.2228e-04, -4.1201e-04, -5.6616e-05],\n",
       "        [-3.6554e-03, -8.5339e-04, -4.1674e-04, -2.3131e-04],\n",
       "        [-3.6731e-03, -8.8651e-04, -4.1540e-04, -1.7479e-04],\n",
       "        [-3.7117e-03, -9.4507e-04, -3.9467e-04, -1.6706e-04]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model.actor(model(b_obs))\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3853, -1.3888, -1.3851, -1.3860, -1.3850, -1.3850, -1.3854, -1.3852,\n",
       "        -1.3859, -1.3887], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = Categorical(logits=logits)\n",
    "probs = probs.log_prob(b_actions)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train-procgen",
   "language": "python",
   "name": "train-procgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
